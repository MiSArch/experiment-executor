# This file is used to define the configuration for the experiment
# The idea is that on user submit in the frontend the config is sent to the experiment config service and the experiment starts
# - first the load profile is executed once without failure to measure the steady state
# - then the failure states are injected and the load profile is executed again
# - finally all metrics are exported and filtered as desired
# The load tests are done with a k6 script that is generated from the experiment config
# The metrics are collected from prometheus


failure:
  infrastructure:
    kubernetes:
      nodes:
        - node1: shutdown
      pods:
        - pod1: shutdown
        - pod2: shutdown
  container:
    availability:
      - shutdown
    latency:
      - path: "/service"
        latency: 1000
    response:
      - path: "/service"
        status: 500
    resources:
      cpu: 60
      memory: 3
  todo:
    123

load:
  graphql:
    flows:
      - name: "Create Order"
        description: "Create a new order"
        maxDurationPerIteration: 100
        maxIterationsPerVirtualUser: 100
        parallelVirtualUsers: 10
        dynamicLoadProfile:
          increasing: 10
            # 10 percent per minute or something -> however those complex load profiles should look like exactly
          duration: 10
        requests:
          - categories:
              output:
                - categoryIds # however to be defined (probably manually in the user interface)
          - products
          - product:
            - input:
              - id
          - users
          - user
            - id
          - shipmentMethods
          - paymentInformations
          - createShoppingCartItem
            - id
            - productVariantId
          - createOrder
            - userId
            - orderItemInputs
            - shipmentAddressId
            - invoiceAddressId
            - paymentInformationId
            - vatNumber
          - placeOrder
            - id
            - cvc


# The GraphQL query script fetches all queries and TODO does return information on the necessary input values
# The tester can the click together the queries and the input values -> represented in a list
# Each request in a flow is executed iteratively, named after the graphQL query and the ids from the response necessary for the future need to be stored

# Might be useful to configure the wanted metrics somehow for the output report
metrics:
    - name: "CPU"
      description: "The CPU usage of the container"
      unit: "percent"
      threshold:
      - "container y consumes x percent of CPU"